import os
import sys
import requests
import uuid
from concurrent.futures import ThreadPoolExecutor

import fitz  # PyMuPDF
from dotenv import load_dotenv
from neo4j import GraphDatabase
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

from qdrant_client import QdrantClient, models
from neo4j_graphrag.retrievers import QdrantNeo4jRetriever

import regolo


class GraphEntry(BaseModel):
    node: str
    target_node: str | None
    relationship: str | None


class GraphComponents(BaseModel):
    graph: list[GraphEntry]



# =========================
# ENV & CLIENTS
# =========================

load_dotenv()  # oppure load_dotenv('.env.local')

qdrant_key = os.getenv("QDRANT_KEY")
qdrant_url = os.getenv("QDRANT_URL")
neo4j_uri = os.getenv("NEO4J_URI")
neo4j_username = os.getenv("NEO4J_USERNAME", "neo4j")
neo4j_password = os.getenv("NEO4J_PASSWORD", "password")
REGOLO_API_URL = "https://api.regolo.ai/v1/chat/completions"
REGOLO_MODEL = "Llama-3.3-70B-Instruct"
REGOLO_KEY = os.getenv("REGOLO_KEY")

neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))

if qdrant_key:
    qdrant_client = QdrantClient(
        url=qdrant_url,
        api_key=qdrant_key,
    )
else:
    qdrant_client = QdrantClient(
        url=qdrant_url,
    )



# Modello di embedding TESTO ‚Üí VETTORE
# (puoi cambiarlo con un altro sentence transformer se vuoi)
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
VECTOR_DIM = embedding_model.get_sentence_embedding_dimension()


# =========================
# HELPER REGOL0
# =========================

def regolo_llm(prompt: str) -> str:
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {REGOLO_KEY}",
    }

    data = {
        "model": REGOLO_MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }

    try:
        resp = requests.post(REGOLO_API_URL, headers=headers, json=data)

        if resp.status_code != 200:
            print(f"[REGOLO ERROR] {resp.status_code}")
            return f"Error {resp.status_code}: {resp.text}"

        j = resp.json()
        answer = j.get("choices", [{}])[0].get("message", {}).get("content", "")

        # üî• PRINT MINIMALE E SICURO
        print(f"[REGOLO OK] ({len(answer)} chars)")

        return answer

    except Exception as e:
        print(f"[REGOLO EXCEPTION] {e}")
        return f"Error calling Regolo HTTP: {str(e)}"




# =========================
# PDF ‚Üí TESTO (+ pseudo-tabelle)
# =========================




def extract_pdf_text_with_tables(pdf_path: str) -> str:
    """
    Estrae tutto il testo del PDF, comprese le tabelle (in forma testuale),
    restituendo un'unica stringa pronta per il chunking.
    """
    doc = fitz.open(pdf_path)
    all_text_blocks = []

    for page_index in range(doc.page_count):
        page = doc[page_index]

        # Testo normale
        page_text = page.get_text("text")

        # Estrazione blocchi: (x0, y0, x1, y1, text, block_no, block_type, ...)
        blocks = page.get_text("blocks")
        table_texts = []

        for b in blocks:
            text_block = b[4]

            # Heuristica: se contiene colonne, pipe, tab ‚Üí possibile tabella
            if "|" in text_block or "\t" in text_block:
                # Normalizzazione per aiutarci nel chunking
                cleaned = text_block.replace("\t", " | ")
                table_texts.append(cleaned)

        if table_texts:
            tables_serialized = "\n[TABELLA]:\n" + "\n".join(table_texts)
            full_page_text = page_text + "\n" + tables_serialized
        else:
            full_page_text = page_text

        all_text_blocks.append(full_page_text)

    doc.close()

    # Restituiamo *tutto* come unico grande testo
    return "\n\n".join(all_text_blocks)

def chunk_text(text: str, max_words: int = 400) -> list[str]:
    words = text.split()
    chunks = []
    current = []

    for w in words:
        current.append(w)
        if len(current) >= max_words:
            chunks.append(" ".join(current))
            current = []

    if current:
        chunks.append(" ".join(current))

    return chunks


# =========================
# ESTR. GRAFO CON REGOLO
# =========================

def regolo_llm_parser(prompt: str) -> GraphComponents:
    internal_prompt = (f"""
    
        Sei un estrattore estremamente preciso di relazioni per grafi di conoscenza.
        DAL TESTO DEVI ESTRARRE IL MAGGIOR NUMERO POSSIBILE DI RELAZIONI CORRETTE TRA ENTIT√Ä.

        Devi restituire ESCLUSIVAMENTE un JSON con la seguente struttura ESATTA:

        {{
        "graph": [
            {{
            "node": "Persona/Entit√† di partenza",
            "target_node": "Entit√† collegata",
            "relationship": "Tipo di relazione"
            }}
        ]
        }}

        REGOLE GENERALI:
        - Usa ESATTAMENTE le chiavi: "graph", "node", "target_node", "relationship".
        - "node" e "target_node" devono essere nomi brevi e leggibili di concetti/entit√†
        (es. "Articolo 2", "Beneficiari", "Requisiti di accesso", "Spesa ammissibile").
        - "relationship" deve essere una breve etichetta testuale (in italiano o inglese)
        che descrive il tipo di relazione (es. "definisce", "regola", "applica a",
        "riguarda", "dipende da", "richiede", "fa parte di", "√® un tipo di").

        OBIETTIVO: MASSIMIZZARE LE RELAZIONI CORRETTE
        - Estrai TUTTE le relazioni esplicite presenti nel testo.
        - Estrai anche relazioni implicite che siano ragionevolmente deducibili dal contesto.
        - Se in una frase compaiono pi√π di due entit√† legate tra loro, crea pi√π relazioni,
        anche multiple, ad esempio:
        - A "definisce" B
        - A "regola" C
        - B "si applica a" C
        - Collega la stessa entit√† anche tra FRASI e PARAGRAFI diversi
        (es. "Articolo 2" collegato a "Beneficiari", "Spese ammissibili", "Requisiti").

        NORMALIZZAZIONE E DEDUPLICA:
        - Se lo stesso concetto appare con varianti di testo simili, usa un UNICO nome coerente
        (es. "articolo 2", "Art. 2" ‚Üí "Articolo 2").
        - EVITA relazioni duplicate identiche
        (stessa "node", stessa "target_node", stessa "relationship").
        - EVITA relazioni banali o tautologiche (es. A -> A, o relazioni senza significato).

        TIPI DI RELAZIONE (ESEMPI NON ESAUSTIVI):
        - gerarchia/struttura: "contiene", "fa parte di", "√® sezione di"
        - definizione: "definisce", "descrive", "stabilisce"
        - applicazione: "si applica a", "riguarda", "vale per"
        - requisito: "richiede", "dipende da", "√® subordinato a"
        - temporale: "decorre da", "termina a", "√® valido fino a"
        - quantitativo/economico: "finanzia", "limita a", "ha importo massimo di"

        VINCOLI:
        - NON aggiungere NESSUN testo fuori dal JSON (nessuna spiegazione, nessun commento).
        - Se davvero NON trovi relazioni, restituisci esattamente:
        {{ "graph": [] }}

        Testo da analizzare:
        {prompt}
        """
    )

    raw = regolo_llm(internal_prompt)
    if not raw:
        return GraphComponents(graph=[])

    # estrazione robusta del JSON
    txt = raw.strip()
    if not txt.startswith("{"):
        i = txt.find("{")
        j = txt.rfind("}")
        if i != -1 and j != -1:
            txt = txt[i:j+1]

    try:
        return GraphComponents.model_validate_json(txt)
    except Exception:
        print("JSON parse error, raw received:\n", raw)
        return GraphComponents(graph=[])





def extract_graph_components(raw_data: str):
    # Prompt per il parser Regolo
    prompt = f"Estrai nodi e relazioni dal seguente testo:\n{raw_data}"

    # Richiama il parser basato su Regolo (che restituisce GraphComponents)
    parsed_response = regolo_llm_parser(prompt)

    # La struttura √®: { "graph": [ { node, target_node, relationship }, ... ] }
    entries = parsed_response.graph

    nodes = {}
    relationships = []

    for entry in entries:
        node = entry.node
        target = entry.target_node
        rel = entry.relationship

        # Id univoci per ogni nodo
        if node not in nodes:
            nodes[node] = str(uuid.uuid4())

        if target and target not in nodes:
            nodes[target] = str(uuid.uuid4())

        # Relazione valida
        if target and rel:
            relationships.append({
                "source": nodes[node],
                "target": nodes[target],
                "type": rel
            })

    return nodes, relationships



def extract_graph_parallel(chunks, max_workers: int = 10):
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for nodes, rels in executor.map(extract_graph_components, chunks):
            results.append((nodes, rels))
    return results

def build_graph_from_chunks(chunks, parallel: bool = False, max_workers: int = 3):
    """
    Unisce i grafi dei singoli chunk in un unico grafo globale.
    Se parallel=True usa ThreadPoolExecutor per chiamare extract_graph_components.
    Ritorna:
    - all_nodes: dict[name -> global_id]
    - all_relationships: lista di relazioni globali
    - chunk_node_mapping: lista di liste, per ogni chunk gli id dei nodi globali creati da quel chunk
    """
    # 1) Estrazione nodi/relazioni dai chunk
    if parallel:
        chunk_graphs = extract_graph_parallel(chunks, max_workers=max_workers)
        # chunk_graphs: list[(nodes_chunk, rels_chunk)]
    else:
        chunk_graphs = [extract_graph_components(chunk) for chunk in chunks]

    # 2) Merge globale
    all_nodes: dict[str, str] = {}
    all_relationships: list[dict] = []
    chunk_node_mapping: list[list[str]] = []

    for nodes_chunk, rels_chunk in chunk_graphs:
        temp_to_final: dict[str, str] = {}

        # Mappo i node_id temporanei di questo chunk su quelli globali
        for name, temp_id in nodes_chunk.items():
            if name not in all_nodes:
                all_nodes[name] = temp_id
            temp_to_final[temp_id] = all_nodes[name]

        # Registra i nodi globali associati a QUESTO chunk
        chunk_node_mapping.append(list(temp_to_final.values()))

        # Riallaccio le relazioni ai node_id globali
        for rel in rels_chunk:
            src_temp = rel["source"]
            tgt_temp = rel["target"]
            rel_type = rel["type"]

            src_final = temp_to_final.get(src_temp)
            tgt_final = temp_to_final.get(tgt_temp)
            if not src_final or not tgt_final:
                continue

            all_relationships.append(
                {"source": src_final, "target": tgt_final, "type": rel_type}
            )

    return all_nodes, all_relationships, chunk_node_mapping

# =========================
# NEO4J INGEST
# =========================

def ingest_to_neo4j(nodes, relationships, driver=None):
    """
    Ingest nodes and relationships into Neo4j 
    Se driver √® None, usa il neo4j_driver globale.
    """
    neo4j_client = driver or neo4j_driver

    with neo4j_client.session() as session:
        # Nodi: CREATE su id, aggiorna sempre il name
        for name, node_id in nodes.items():
            session.run(
                """
                CREATE (n:Entity {id: $id})
                SET n.name = $name
                """,
                id=node_id,
                name=name,
            )

        # Relazioni: CREATE per evitare duplicati
        for relationship in relationships:
            session.run(
                """
                MATCH (a:Entity {id: $source_id}), (b:Entity {id: $target_id})
                CREATE (a)-[r:RELATIONSHIP {type: $type}]->(b)
                """,
                source_id=relationship["source"],
                target_id=relationship["target"],
                type=relationship["type"],
            )

    return nodes

# =========================
# QDRANT COLLECTION & INGEST
# =========================

def create_collection(client, collection_name, vector_dimension):
    """
    Crea la collection Qdrant solo se non esiste.
    """
    try:
        client.get_collection(collection_name)
        print(f"La collection '{collection_name}' esiste gi√†. Nessuna creazione necessaria.")
        return

    except Exception as e:
        # Caso previsto: collection inesistente
        if "Not found" in str(e):
            print(f"La collection '{collection_name}' non esiste. Creazione in corso...")

            client.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=vector_dimension,
                    distance=models.Distance.COSINE
                )
            )

            print(f"Collection '{collection_name}' creata correttamente.")
            return

        # Caso non previsto: errore reale
        print(f"Errore durante il controllo della collection: {e}")



def ingest_to_qdrant(
    collection_name: str,
    chunks: list[str],
    chunk_node_mapping: list[list[str]],
):
    """
    Ingestione in Qdrant:
    - chunks: lista di stringhe (testo per chunk)
    - chunk_node_mapping: per ogni chunk, lista di node_id globali associati

    Ogni punto in Qdrant rappresenta un chunk e conosce i nodi Neo4j a cui √® collegato.
    """
    embeddings = embedding_model.encode(chunks, convert_to_numpy=True)

    points = []
    for idx, (emb, node_ids) in enumerate(zip(embeddings, chunk_node_mapping)):
        payload = {
            "text": chunks[idx],
            "nodes": node_ids,  # lista di id Neo4j collegati
        }

        # üëá aggiungiamo un id "principale" per il retriever
        if node_ids:
            payload["id"] = node_ids[0]  # deve corrispondere a Entity.id in Neo4j

        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector=emb.tolist(),
                payload=payload,
            )
        )


    qdrant_client.upsert(
        collection_name=collection_name,
        points=points,
    )

    print(f"Inseriti {len(points)} punti in Qdrant.")
    return len(points)



def generate_query_embedding(query: str):
    emb = embedding_model.encode([query], convert_to_numpy=True)[0]
    return emb.tolist()


def retriever_search(
    neo4j_driver,
    qdrant_client,
    collection_name: str,
    query: str,
):
    retriever = QdrantNeo4jRetriever(
        driver=neo4j_driver,
        client=qdrant_client,
        collection_name=collection_name,
        id_property_external="id",
        id_property_neo4j="id",
    )

    query_vector = generate_query_embedding(query)

    results = retriever.search(
        query_vector=query_vector,
        top_k=5,
    )

    return results


# =========================
# GRAPH CONTEXT & QA CON REGOLO
# =========================

def fetch_related_graph(neo4j_client, entity_ids):
    query = """
    MATCH (e:Entity)-[r1]-(n1)-[r2]-(n2)
    WHERE e.id IN $entity_ids
    RETURN e, r1 as r, n1 as related, r2, n2
    UNION
    MATCH (e:Entity)-[r]-(related)
    WHERE e.id IN $entity_ids
    RETURN e, r, related, null as r2, null as n2
    """
    with neo4j_client.session() as session:
        result = session.run(query, entity_ids=entity_ids)
        subgraph = []
        for record in result:
            subgraph.append(
                {
                    "entity": record["e"],
                    "relationship": record["r"],
                    "related_node": record["related"],
                }
            )
            if record["r2"] and record["n2"]:
                subgraph.append(
                    {
                        "entity": record["related"],
                        "relationship": record["r2"],
                        "related_node": record["n2"],
                    }
                )
    return subgraph


def format_graph_context(subgraph):
    nodes = set()
    edges = []

    for entry in subgraph:
        entity = entry["entity"]
        related = entry["related_node"]
        relationship = entry["relationship"]

        nodes.add(entity["name"])
        nodes.add(related["name"])

        edges.append(f"{entity['name']} {relationship['type']} {related['name']}")

    return {"nodes": list(nodes), "edges": edges}


def graphRAG_run(graph_context, user_query: str) -> str:
    nodes_str = ", ".join(graph_context["nodes"])
    edges_str = "; ".join(graph_context["edges"])
    prompt = f"""
Sei un assistente che risponde usando SOLO il seguente grafo di conoscenza.

NODI:
{nodes_str}

ARCHI:
{edges_str}

Domanda dell'utente:
\"{user_query}\"

Rispondi in modo sintetico, preciso e aderente al grafo.
"""
    response = regolo_llm(prompt)
    return response

if __name__ == "__main__":
    print("Script started")
    print("Loading environment variables...")
    # (gi√† fatto sopra con load_dotenv('.env.local'), qui solo log)
    print("Environment variables loaded")
    
    print("Initializing clients...")
    # (gi√† inizializzati sopra, qui solo log)
    print("Clients initialized")
    
    print("Creating collection...")
    collection_name = "Bandi"
    vector_dimension = VECTOR_DIM
    create_collection(qdrant_client, collection_name, vector_dimension)
    print("Collection created/verified")
    
    print("Extracting graph components...")


    doc_path = "DD_G13041_10_10_2025_Bando.pdf"

    print("Reading PDF and extracting text + tables...")
    raw_data = extract_pdf_text_with_tables(doc_path)

    print("Chunking raw_data...")
    chunks = chunk_text(raw_data, max_words=1000)

    print(f"Chunks extracted: {len(chunks)}")
    if not chunks:
        print("No chunks extracted. Exiting.")
        sys.exit(0)

    # ================
    # 2) GRAFO DAI CHUNK
    # ================
    nodes, relationships, chunk_node_mapping = build_graph_from_chunks(
    chunks, parallel=True, max_workers=3
)

    print("Nodes:", nodes)
    print("Relationships:", relationships)
    
    # ================
    # 3) NEO4J
    # ================
    print("Ingesting to Neo4j...")
    node_id_mapping = ingest_to_neo4j(nodes, relationships)
    print("Neo4j ingestion complete")
    
    # ================
    # 4) QDRANT
    # ================
    print("Ingesting to Qdrant...")
    # ora passiamo la LISTA DI CHUNK, non la stringa intera
    ingest_to_qdrant(collection_name, chunks, chunk_node_mapping)
    print("Qdrant ingestion complete")

    # ================
    # 5) RETRIEVAL + GRAPHRAG
    # ================



    query = "Chi pu√≤ applicare per questo bando?"



    print("Starting retriever search...")
    retriever_result = retriever_search(neo4j_driver, qdrant_client, collection_name, query)
    print("Retriever results:", retriever_result)
    
    print("Extracting entity IDs...")
    entity_ids = []
    for item in getattr(retriever_result, "items", []):
        try:
            entity_id = item.content.split("'id': '")[1].split("'")[0]
            entity_ids.append(entity_id)
        except Exception:
            continue
    print("Entity IDs:", entity_ids)
    
    if not entity_ids:
        print("No entity IDs found. Stopping before subgraph/GraphRAG.")
        sys.exit(0)

    print("Fetching related graph...")
    subgraph = fetch_related_graph(neo4j_driver, entity_ids)
    print("Subgraph:", subgraph)
    
    print("Formatting graph context...")
    graph_context = format_graph_context(subgraph)
    print("Graph context:", graph_context)
    
    print("Running GraphRAG...")
    answer = graphRAG_run(graph_context, query)
    print("Final Answer:", answer)






    import os
import sys
import uuid
from concurrent.futures import ThreadPoolExecutor

import fitz  # PyMuPDF
from dotenv import load_dotenv
from neo4j import GraphDatabase
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

from qdrant_client import QdrantClient, models
from neo4j_graphrag.retrievers import QdrantNeo4jRetriever

from celery_app import regolo_call


class GraphEntry(BaseModel):
    node: str
    target_node: str | None
    relationship: str | None


class GraphComponents(BaseModel):
    graph: list[GraphEntry]



# =========================
# ENV & CLIENTS
# =========================

load_dotenv()  # oppure load_dotenv('.env.local')

qdrant_key = os.getenv("QDRANT_KEY")
qdrant_url = os.getenv("QDRANT_URL")
neo4j_uri = os.getenv("NEO4J_URI")
neo4j_username = os.getenv("NEO4J_USERNAME", "neo4j")
neo4j_password = os.getenv("NEO4J_PASSWORD", "password")


neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))

if qdrant_key:
    qdrant_client = QdrantClient(
        url=qdrant_url,
        api_key=qdrant_key,
    )
else:
    qdrant_client = QdrantClient(
        url=qdrant_url,
    )



# Modello di embedding TESTO ‚Üí VETTORE
# (puoi cambiarlo con un altro sentence transformer se vuoi)
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
VECTOR_DIM = embedding_model.get_sentence_embedding_dimension()


# =========================
# HELPER REGOL0
# =========================




# =========================
# PDF ‚Üí TESTO (+ pseudo-tabelle)
# =========================

def build_internal_prompt(prompt: str) -> str:
    return f"""
Sei un estrattore estremamente preciso di relazioni per grafi di conoscenza.
DAL TESTO DEVI ESTRARRE IL MAGGIOR NUMERO POSSIBILE DI RELAZIONI CORRETTE TRA ENTIT√Ä.

Devi restituire ESCLUSIVAMENTE un JSON con la seguente struttura ESATTA:

{{
  "graph": [
    {{
      "node": "Persona/Entit√† di partenza",
      "target_node": "Entit√† collegata",
      "relationship": "Tipo di relazione"
    }}
  ]
}}

REGOLE GENERALI:
- Usa ESATTAMENTE le chiavi: "graph", "node", "target_node", "relationship".
- "node" e "target_node" devono essere nomi brevi e leggibili di concetti/entit√†
  (es. "Articolo 2", "Beneficiari", "Requisiti di accesso", "Spesa ammissibile").
- "relationship" deve essere una breve etichetta testuale (in italiano o inglese)
  che descrive il tipo di relazione (es. "definisce", "regola", "applica a",
  "riguarda", "dipende da", "richiede", "fa parte di", "√® un tipo di").

...

Testo da analizzare:
{prompt}
"""

def parse_graph(raw: str):
    txt = raw.strip()

    if not txt.startswith("{"):
        i = txt.find("{")
        j = txt.rfind("}")
        if i != -1 and j != -1:
            txt = txt[i:j+1]

    try:
        parsed = GraphComponents.model_validate_json(txt)
        return parsed.graph
    except Exception:
        print("JSON parse error, raw received:\n", raw)
        return []


def extract_pdf_text_with_tables(pdf_path: str) -> str:
    """
    Estrae tutto il testo del PDF, comprese le tabelle (in forma testuale),
    restituendo un'unica stringa pronta per il chunking.
    """
    doc = fitz.open(pdf_path)
    all_text_blocks = []

    for page_index in range(doc.page_count):
        page = doc[page_index]

        # Testo normale
        page_text = page.get_text("text")

        # Estrazione blocchi: (x0, y0, x1, y1, text, block_no, block_type, ...)
        blocks = page.get_text("blocks")
        table_texts = []

        for b in blocks:
            text_block = b[4]

            # Heuristica: se contiene colonne, pipe, tab ‚Üí possibile tabella
            if "|" in text_block or "\t" in text_block:
                # Normalizzazione per aiutarci nel chunking
                cleaned = text_block.replace("\t", " | ")
                table_texts.append(cleaned)

        if table_texts:
            tables_serialized = "\n[TABELLA]:\n" + "\n".join(table_texts)
            full_page_text = page_text + "\n" + tables_serialized
        else:
            full_page_text = page_text

        all_text_blocks.append(full_page_text)

    doc.close()

    # Restituiamo *tutto* come unico grande testo
    return "\n\n".join(all_text_blocks)

def chunk_text(text: str, max_words: int = 400) -> list[str]:
    words = text.split()
    chunks = []
    current = []

    for w in words:
        current.append(w)
        if len(current) >= max_words:
            chunks.append(" ".join(current))
            current = []

    if current:
        chunks.append(" ".join(current))

    return chunks


# =========================
# ESTR. GRAFO CON REGOLO
# =========================

def regolo_llm_parser(text: str):
    internal_prompt = build_internal_prompt(text)
    task = regolo_call.delay(internal_prompt)
    return task




def extract_graph_components(task):
    raw = task.get(timeout=400)

    # MESSAGGIO DI CONFERMA
    print(f"[REGOLO OK] Risposta ricevuta (lunghezza={len(raw)} caratteri)")

    graph = parse_graph(raw)

    # Merge identico al tuo
    nodes_chunk = {}
    relationships_chunk = []

    for entry in graph:
        node = entry.node
        target = entry.target_node
        rel = entry.relationship

        if node not in nodes_chunk:
            nodes_chunk[node] = str(uuid.uuid4())

        if target and target not in nodes_chunk:
            nodes_chunk[target] = str(uuid.uuid4())

        relationships_chunk.append({
            "node": node,
            "target_node": target,
            "relationship": rel
        })

    return nodes_chunk, relationships_chunk

def build_graph_from_chunks(chunks):
    # 1) SCHEDULO TUTTI I TASK CELERY
    tasks = []
    for chunk in chunks:
        task = regolo_llm_parser(chunk)
        tasks.append(task)

    # 2) RACCOLGO RISPOSTE DI CELERY
    chunk_graphs = []
    for task in tasks:
        nodes_chunk, rels_chunk = extract_graph_components(task)
        chunk_graphs.append((nodes_chunk, rels_chunk))

    # 3) MERGE IDENTICO AL TUO ‚Äî NON TOCCO NULLA
    chunk_node_mapping = []
    all_nodes: dict[str, str] = {}
    all_relationships: list[dict] = []

    for nodes_chunk, rels_chunk in chunk_graphs:
        temp_to_final: dict[str, str] = {}
        chunk_global_ids: list[str] = []

        for name, temp_id in nodes_chunk.items():
            if name not in all_nodes:
                all_nodes[name] = temp_id
            temp_to_final[temp_id] = all_nodes[name]
            chunk_global_ids.append(all_nodes[name])

        chunk_node_mapping.append(chunk_global_ids)

        for rel in rels_chunk:
            src = rel["node"]
            tgt = rel["target_node"]
            relationship = rel["relationship"]

            src_id = temp_to_final[nodes_chunk[src]]
            tgt_id = temp_to_final[nodes_chunk[tgt]] if tgt else None

            all_relationships.append({
                "source": src_id,
                "target": tgt_id,
                "type": relationship
            })

    return all_nodes, all_relationships, chunk_node_mapping


# =========================
# NEO4J INGEST
# =========================

def ingest_to_neo4j(nodes, relationships, driver=None):
    """
    Ingest nodes and relationships into Neo4j 
    Se driver √® None, usa il neo4j_driver globale.
    """
    neo4j_client = driver or neo4j_driver

    with neo4j_client.session() as session:
        # Nodi: CREATE su id, aggiorna sempre il name
        for name, node_id in nodes.items():
            session.run(
                """
                CREATE (n:Entity {id: $id})
                SET n.name = $name
                """,
                id=node_id,
                name=name,
            )

        # Relazioni: CREATE per evitare duplicati
        for relationship in relationships:
            session.run(
                """
                MATCH (a:Entity {id: $source_id}), (b:Entity {id: $target_id})
                CREATE (a)-[r:RELATIONSHIP {type: $type}]->(b)
                """,
                source_id=relationship["source"],
                target_id=relationship["target"],
                type=relationship["type"],
            )

    return nodes

# =========================
# QDRANT COLLECTION & INGEST
# =========================

def create_collection(client, collection_name, vector_dimension):
    """
    Crea la collection Qdrant solo se non esiste.
    """
    try:
        client.get_collection(collection_name)
        print(f"La collection '{collection_name}' esiste gi√†. Nessuna creazione necessaria.")
        return

    except Exception as e:
        # Caso previsto: collection inesistente
        if "Not found" in str(e):
            print(f"La collection '{collection_name}' non esiste. Creazione in corso...")

            client.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=vector_dimension,
                    distance=models.Distance.COSINE
                )
            )

            print(f"Collection '{collection_name}' creata correttamente.")
            return

        # Caso non previsto: errore reale
        print(f"Errore durante il controllo della collection: {e}")



def ingest_to_qdrant(
    collection_name: str,
    chunks: list[str],
    chunk_node_mapping: list[list[str]],
):
    """
    Ingestione in Qdrant:
    - chunks: lista di stringhe (testo per chunk)
    - chunk_node_mapping: per ogni chunk, lista di node_id globali associati

    Ogni punto in Qdrant rappresenta un chunk e conosce i nodi Neo4j a cui √® collegato.
    """
    embeddings = embedding_model.encode(chunks, convert_to_numpy=True)

    points = []
    for idx, (emb, node_ids) in enumerate(zip(embeddings, chunk_node_mapping)):
        payload = {
            "text": chunks[idx],
            "nodes": node_ids,  # lista di id Neo4j collegati
        }

        # üëá aggiungiamo un id "principale" per il retriever
        if node_ids:
            payload["id"] = node_ids[0]  # deve corrispondere a Entity.id in Neo4j

        points.append(
            models.PointStruct(
                id=str(uuid.uuid4()),
                vector=emb.tolist(),
                payload=payload,
            )
        )


    qdrant_client.upsert(
        collection_name=collection_name,
        points=points,
    )

    print(f"Inseriti {len(points)} punti in Qdrant.")
    return len(points)



def generate_query_embedding(query: str):
    emb = embedding_model.encode([query], convert_to_numpy=True)[0]
    return emb.tolist()


def retriever_search(
    neo4j_driver,
    qdrant_client,
    collection_name: str,
    query: str,
):
    retriever = QdrantNeo4jRetriever(
        driver=neo4j_driver,
        client=qdrant_client,
        collection_name=collection_name,
        id_property_external="id",
        id_property_neo4j="id",
    )

    query_vector = generate_query_embedding(query)

    results = retriever.search(
        query_vector=query_vector,
        top_k=5,
    )

    return results


# =========================
# GRAPH CONTEXT & QA CON REGOLO
# =========================

def fetch_related_graph(neo4j_client, entity_ids):
    query = """
    MATCH (e:Entity)-[r1]-(n1)-[r2]-(n2)
    WHERE e.id IN $entity_ids
    RETURN e, r1 as r, n1 as related, r2, n2
    UNION
    MATCH (e:Entity)-[r]-(related)
    WHERE e.id IN $entity_ids
    RETURN e, r, related, null as r2, null as n2
    """
    with neo4j_client.session() as session:
        result = session.run(query, entity_ids=entity_ids)
        subgraph = []
        for record in result:
            subgraph.append(
                {
                    "entity": record["e"],
                    "relationship": record["r"],
                    "related_node": record["related"],
                }
            )
            if record["r2"] and record["n2"]:
                subgraph.append(
                    {
                        "entity": record["related"],
                        "relationship": record["r2"],
                        "related_node": record["n2"],
                    }
                )
    return subgraph


def format_graph_context(subgraph):
    nodes = set()
    edges = []

    for entry in subgraph:
        entity = entry["entity"]
        related = entry["related_node"]
        relationship = entry["relationship"]

        nodes.add(entity["name"])
        nodes.add(related["name"])

        edges.append(f"{entity['name']} {relationship['type']} {related['name']}")

    return {"nodes": list(nodes), "edges": edges}


def graphRAG_run(graph_context, user_query: str) -> str:
    nodes_str = ", ".join(graph_context["nodes"])
    edges_str = "; ".join(graph_context["edges"])
    prompt = f"""
Sei un assistente che risponde usando SOLO il seguente grafo di conoscenza.

NODI:
{nodes_str}

ARCHI:
{edges_str}

Domanda dell'utente:
\"{user_query}\"

Rispondi in modo sintetico, preciso e aderente al grafo.
"""
    task = regolo_call.delay(prompt)
    response = task.get(timeout=400)
    
    return response


if __name__ == "__main__":
    print("Script started")
    print("Loading environment variables...")
    # (gi√† fatto sopra con load_dotenv('.env.local'), qui solo log)
    print("Environment variables loaded")
    
    print("Initializing clients...")
    # (gi√† inizializzati sopra, qui solo log)
    print("Clients initialized")
    
    print("Creating collection...")
    collection_name = "Bandi"
    vector_dimension = VECTOR_DIM
    create_collection(qdrant_client, collection_name, vector_dimension)
    print("Collection created/verified")
    
    print("Extracting graph components...")


    doc_path = "DD_G13041_10_10_2025_Bando.pdf"


    print("Reading PDF and extracting text + tables...")
    raw_data = extract_pdf_text_with_tables(doc_path)

    print("Chunking raw_data...")
    chunks = chunk_text(raw_data, max_words=650)

    print(f"Chunks extracted: {len(chunks)}")
    if not chunks:
        print("No chunks extracted. Exiting.")
        sys.exit(0)

    # ================
    # 2) GRAFO DAI CHUNK
    # ================
    nodes, relationships, chunk_node_mapping = build_graph_from_chunks(chunks)

    print("Nodes:", nodes)
    print("Relationships:", relationships)
    
    # ================
    # 3) NEO4J
    # ================
    print("Ingesting to Neo4j...")
    node_id_mapping = ingest_to_neo4j(nodes, relationships)
    print("Neo4j ingestion complete")
    
    # ================
    # 4) QDRANT
    # ================
    print("Ingesting to Qdrant...")
    # ora passiamo la LISTA DI CHUNK, non la stringa intera
    ingest_to_qdrant(collection_name, chunks, chunk_node_mapping)
    print("Qdrant ingestion complete")

    # ================
    # 5) RETRIEVAL + GRAPHRAG
    # ================



    query = "Chi pu√≤ applicare per questo bando?"



    print("Starting retriever search...")
    retriever_result = retriever_search(neo4j_driver, qdrant_client, collection_name, query)
    print("Retriever results:", retriever_result)
    
    print("Extracting entity IDs...")
    entity_ids = []
    for item in getattr(retriever_result, "items", []):
        try:
            entity_id = item.content.split("'id': '")[1].split("'")[0]
            entity_ids.append(entity_id)
        except Exception:
            continue
    print("Entity IDs:", entity_ids)
    
    if not entity_ids:
        print("No entity IDs found. Stopping before subgraph/GraphRAG.")
        sys.exit(0)

    print("Fetching related graph...")
    subgraph = fetch_related_graph(neo4j_driver, entity_ids)
    print("Subgraph:", subgraph)
    
    print("Formatting graph context...")
    graph_context = format_graph_context(subgraph)
    print("Graph context:", graph_context)
    
    print("Running GraphRAG...")
    answer = graphRAG_run(graph_context, query)
    print("Final Answer:", answer)